{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline('summarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Text mining, also referred to as text data mining (abbr.: TDM), similar to text analytics, \n",
    "        is the process of deriving high-quality information from text. It involves \n",
    "        \"the discovery by computer of new, previously unknown information, \n",
    "        by automatically extracting information from different written resources.\" \n",
    "        Written resources may include websites, books, emails, reviews, and articles. \n",
    "        High-quality information is typically obtained by devising patterns and trends \n",
    "        by means such as statistical pattern learning. According to Hotho et al. (2005)\n",
    "        we can distinguish between three different perspectives of text mining: \n",
    "        information extraction, data mining, and a KDD (Knowledge Discovery in Databases) process.''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' Text mining involves deriving high-quality information from text . Written resources may include websites, books, emails, reviews, and articles . Text mining is similar to text analytics . It involves the discovery by computer of new, previously unknown information by automatically extracting information from different written resources .'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = summarizer(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(778, 341)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text), len(result[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\", model_max_length=512)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('t5-small')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.models.t5.tokenization_t5_fast.T5TokenizerFast,\n",
       " transformers.models.t5.modeling_t5.T5ForConditionalGeneration)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer), type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
    "input_text = \"summarize: \" + preprocess_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ids = model.generate(tokenized_text,\n",
    "                             num_beams=4,\n",
    "                             no_repeat_ngram_size=3,\n",
    "                             min_length=30,\n",
    "                             max_length=100,\n",
    "                             early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text data mining is the process of deriving high-quality information from text. it involves the discovery by computer of new, previously unknown information. a KDD (Knowledge Discovery in Databases) process is similar to text analytics.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5TokenizerFast.from_pretrained('t5-small', model_max_length=1024)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''The Inflation Reduction Act lowers prescription drug costs, health care costs, \n",
    "and energy costs. It's the most aggressive action on tackling the climate crisis in American history, \n",
    "which will lift up American workers and create good-paying, union jobs across the country. \n",
    "It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. \n",
    "And no one making under $400,000 per year will pay a penny more in taxes.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_text = text.strip().replace('\\n', '')\n",
    "input_text = 'summarize: ' + preprocess_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "summary_ids = model.generate(tokenized_text,\n",
    "                             num_beams=4,\n",
    "                             no_repeat_ngram_size=3,\n",
    "                             min_length=30,\n",
    "                             max_length=100,\n",
    "                             early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. it's the most aggressive action on tackling the climate crisis in history. no one making under $400,000 per year will pay a penny more in taxes.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'The people of the State of California do enact as follows:\\n\\n\\nSECTION 1.\\nSection 12012.75 of the Government Code is amended to read:\\n12012.75.\\nThere is hereby created in the State Treasury a special fund called the “Indian Gaming Revenue Sharing Trust Fund” for the receipt and deposit of moneys received by the state from Indian tribes pursuant to the terms of tribal-state gaming compacts for the purpose of making distributions to eligible recipient Indian tribes. Moneys in the Indian Gaming Revenue Sharing Trust Fund shall be available to the California Gambling Control Commission, upon appropriation by the Legislature, for the purpose of making distributions to eligible recipient Indian tribes, in accordance with distribution plans specified in tribal-state gaming compacts.\\nSEC. 2.\\nSection 12012.90 of the Government Code is amended to read:\\n12012.90.\\nFor each fiscal year commencing with the 2016–17 fiscal year, all of the following shall apply:\\n(a) On or before the day of the May budget revision for each fiscal year, the California Gambling Control Commission shall determine the anticipated total amount of shortfalls in payment likely to occur in the Indian Gaming Revenue Sharing Trust Fund for the next fiscal year, and shall provide to the committee in the Senate and Assembly that considers the State Budget an estimate of the amount needed to transfer from the Indian Gaming Special Distribution Fund to backfill the Indian Gaming Revenue Sharing Trust Fund for the next fiscal year. The anticipated total amount of shortfalls to be transferred from the Indian Gaming Special Distribution Fund to the Indian Gaming Revenue Sharing Trust Fund shall be determined by the California Gambling Control Commission as follows:\\n(1) The anticipated number of eligible recipient Indian tribes that will be eligible to receive payments for the next fiscal year, multiplied by one million one hundred thousand dollars ($1,100,000), with that product reduced by the amount anticipated to be paid by the tribes directly into the Indian Gaming Revenue Sharing Trust Fund for the next fiscal year.\\n(2) For purposes of this section and Section 12012.75, “eligible recipient Indian tribe” means a noncompact, nongaming, or limited-gaming tribe, as defined in the tribal-state gaming compacts ratified and in effect as provided in subdivision (f) of Section 19 of Article IV of the California Constitution.\\n(3) This amount shall be based upon actual payments received into the Indian Gaming Revenue Sharing Trust Fund the previous fiscal year, with adjustments made due to amendments to existing tribal-state gaming compacts or newly executed tribal-state gaming compacts with respect to payments to be made to the Indian Gaming Revenue Sharing Trust Fund.\\n(b) The Legislature shall transfer from the Indian Gaming Special Distribution Fund to the Indian Gaming Revenue Sharing Trust Fund an amount sufficient for each eligible recipient Indian tribe to receive a total not to exceed two hundred seventy-five thousand dollars ($275,000) for each quarter in the next fiscal year that an eligible recipient Indian tribe is eligible to receive moneys, for a total not to exceed one million one hundred thousand dollars ($1,100,000) for the entire fiscal year. The California Gambling Control Commission shall make quarterly payments from the Indian Gaming Revenue Sharing Trust Fund to each eligible recipient Indian tribe within 45 days of the end of each fiscal quarter.\\n(c) If the transfer of funds from the Indian Gaming Special Distribution Fund to the Indian Gaming Revenue Sharing Trust Fund results in a surplus, the funds shall remain in the Indian Gaming Revenue Sharing Trust Fund for disbursement in future years, and if necessary, adjustments shall be made to future distributions from the Indian Gaming Special Distribution Fund to the Revenue Sharing Trust Fund.\\n(d) In the event the amount appropriated for the fiscal year is insufficient to ensure each eligible recipient Indian tribe receives the total of two hundred seventy-five thousand dollars ($275,000) for each fiscal quarter, the Department of Finance, after consultation with the California Gambling Control Commission, shall submit to the Legislature a request for a budget augmentation for the current fiscal year with an explanation as to the reason why the amount appropriated for the fiscal year was insufficient.\\n(e) At the end of each fiscal quarter, the California Gambling Control Commission’s Indian Gaming Revenue Sharing Trust Fund report shall include information that identifies each of the eligible recipient Indian tribes for that fiscal quarter, the amount paid into the Indian Gaming Revenue Sharing Trust Fund by each of the tribes pursuant to the applicable sections of the tribal-state gaming compact, provided that tribes contributing on a net win or gross gaming revenue basis may be aggregated in the report, and the amount necessary to backfill from the Indian Gaming Special Distribution Fund the shortfall in the Indian Gaming Revenue Sharing Trust Fund in order for each eligible recipient Indian tribe to receive the total of two hundred seventy-five thousand dollars ($275,000) for the fiscal quarter.',\n",
       " 'summary': 'Existing federal law, the Indian Gaming Regulatory Act of 1988, provides for the negotiation and execution of tribal-state gaming compacts for the purpose of authorizing certain types of gaming on Indian lands within a state. The California Constitution authorizes the Governor to negotiate and conclude compacts, subject to ratification by the Legislature. Existing law expressly ratifies a number of tribal-state gaming compacts, and amendments of tribal-state gaming compacts, between the State of California and specified Indian tribes.\\nExisting law establishes the Indian Gaming Revenue Sharing Trust Fund within the State Treasury for the receipt and deposit of moneys derived from gaming device license fees that are received from tribes pursuant to the terms of tribal-state gaming compacts for the purpose of making distributions to noncompact tribes. Existing law provides that moneys in that fund are available to the California Gambling Control Commission, upon appropriation by the Legislature, for the purpose of making those distributions in accordance with plans specified in tribal-state gaming compacts.\\nThis bill would clarify that the purpose of the fund is for making distributions to eligible recipient Indian tribes.\\nExisting law requires the California Gambling Control Commission to, on or before the day of the May budget revision for each fiscal year, determine the anticipated total amount of shortfalls in payment likely to occur in the Indian Gaming Revenue Sharing Trust Fund for the next fiscal year, and to provide to the committee in the Senate and Assembly that considers the State Budget an estimate of the amount needed to transfer from the Indian Gaming Special Distribution Fund to backfill the Indian Gaming Revenue Sharing Trust Fund for the next fiscal year. Existing law requires, at the end of each fiscal quarter, the commission’s Indian Gaming Revenue Sharing Trust Fund report to include specified information, including, among other things, the amount paid into the Indian Gaming Revenue Sharing Trust Fund by each of the tribes pursuant to the applicable sections of the tribal-state gaming compact.\\nThis bill would provide that tribes contributing to the Indian Gaming Revenue Sharing Trust Fund on a net win or gross gaming revenue basis may be aggregated in the quarterly report described above.\\nExisting law requires the California Gambling Control Commission to determine the amount of money needed to be transferred from the Indian Gaming Special Distribution Fund to the Indian Gaming Revenue Sharing Trust Fund to ensure that each eligible recipient Indian tribe receives a specified amount of the funds. Existing law defines “eligible recipient tribe” for those purposes to mean a noncompact tribe, as defined in the tribal-state gaming compacts ratified and in effect, as specified. Those compacts define “noncompact tribe” to mean a federally recognized tribe that operates fewer than 350 gaming devices.\\nThis bill would clarify that “eligible recipient Indian tribe” means a noncompact, nongaming, or limited-gaming tribe, as defined in the tribal-state gaming compacts ratified and in effect, as provided. The bill would delete other related, obsolete provisions.',\n",
       " 'title': 'An act to amend Sections 12012.75 and 12012.90 of the Government Code, relating to gaming.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "billsum = load_dataset(\"billsum\", split=\"ca_test\")\n",
    "billsum = billsum.train_test_split(test_size=0.2)\n",
    "example = billsum[\"train\"][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An act to amend Sections 12012.75 and 12012.90 of the Government Code, relating to gaming.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(data):\n",
    "    inputs = [\"summarize: \" + doc for doc in data[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "    labels = tokenizer(data[\"summary\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6003b74a95347dcb812a36a61f55b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/989 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2cabb1c2e1472bbd5baeade04713a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/248 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 989\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 248\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_billsum = billsum.map(preprocess_text, batched=True, remove_columns=billsum[\"train\"].column_names)\n",
    "tokenized_billsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec362c0028f84613a2bf1dd6f728b7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./summary\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_billsum[\"train\"],\n",
    "    eval_dataset=tokenized_billsum[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 989\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 124\n",
      "  Number of trainable parameters = 60506624\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f922c2c8b8461e87ccfeda009852e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 248\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad14932c98d4a10be4821e0b1c78b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.857461929321289, 'eval_rouge1': 0.1304, 'eval_rouge2': 0.0391, 'eval_rougeL': 0.1083, 'eval_rougeLsum': 0.1085, 'eval_runtime': 389.279, 'eval_samples_per_second': 0.637, 'eval_steps_per_second': 0.041, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 248\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4b1c28ab2d491892516581719650ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.7139813899993896, 'eval_rouge1': 0.1298, 'eval_rouge2': 0.0388, 'eval_rougeL': 0.1056, 'eval_rougeLsum': 0.1058, 'eval_runtime': 386.0763, 'eval_samples_per_second': 0.642, 'eval_steps_per_second': 0.041, 'epoch': 2.0}\n",
      "{'train_runtime': 5574.0904, 'train_samples_per_second': 0.355, 'train_steps_per_second': 0.022, 'train_loss': 3.3591325821415072, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=124, training_loss=3.3591325821415072, metrics={'train_runtime': 5574.0904, 'train_samples_per_second': 0.355, 'train_steps_per_second': 0.022, 'train_loss': 3.3591325821415072, 'epoch': 2.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ids = model.generate(tokenized_text,\n",
    "                             num_beams=4,\n",
    "                             no_repeat_ngram_size=3,\n",
    "                             min_length=30,\n",
    "                             max_length=100,\n",
    "                             early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. it's the most aggressive action on tackling the climate crisis in history. no one making under $400,000 per year will pay a penny more in taxes.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
