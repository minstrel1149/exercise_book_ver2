{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "from konlpy.tag import Okt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \"Hello everyone. It's good to see you. Let's start our text mining class!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello everyone.',\n",
       " \"It's good to see you.\",\n",
       " \"Let's start our text mining class!\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_kor = \"안녕하세요, 여러분. 만나서 반갑습니다. 이제 텍스트마이닝 클래스를 시작해봅시다!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요, 여러분.', '만나서 반갑습니다.', '이제 텍스트마이닝 클래스를 시작해봅시다!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(para_kor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'everyone',\n",
       " '.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'good',\n",
       " 'to',\n",
       " 'see',\n",
       " 'you',\n",
       " '.',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'start',\n",
       " 'our',\n",
       " 'text',\n",
       " 'mining',\n",
       " 'class',\n",
       " '!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'everyone',\n",
       " '.',\n",
       " 'It',\n",
       " \"'\",\n",
       " 's',\n",
       " 'good',\n",
       " 'to',\n",
       " 'see',\n",
       " 'you',\n",
       " '.',\n",
       " 'Let',\n",
       " \"'\",\n",
       " 's',\n",
       " 'start',\n",
       " 'our',\n",
       " 'text',\n",
       " 'mining',\n",
       " 'class',\n",
       " '!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordPunctTokenizer().tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('''[abc]''', '''How are you, boy?''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '7', '5', '9']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('''[0123456789]''', '''3a7b5c9d''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', 'a', '7', 'b', '_', '5', 'c', '9', 'd']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('''[\\w]''', '''3a 7b_ '.^&5c9d''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_', '__', '___']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('''[_]+''', '''a_b, c__d, e___f''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How', 'are', 'you', 'boy']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('''[\\w]+''', '''How are you, boy?''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oo', 'oooo', 'oooo', 'ooo']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('''[o]{2,4}''', '''oh, hoow are yoooou, boooooooy?''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sorry', 'I', \"can't\", 'go', 'there']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer('''[\\w']+''')\n",
    "tokenizer.tokenize(\"Sorry, I can't go there.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sorry', 'i', \"can't\", 'go', 'there']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"Sorry, I can't go there.\"\n",
    "tokenizer.tokenize(text1.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sorry', 'go', 'movie', 'yesterday']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"Sorry, I couldn't go to movie yesterday.\"\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokens = tokenizer.tokenize(text1.lower())\n",
    "result = [word for word in tokens if word not in english_stops]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cook', 'cookeri', 'cookbook')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'everyon',\n",
       " '.',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'good',\n",
       " 'to',\n",
       " 'see',\n",
       " 'you',\n",
       " '.',\n",
       " 'let',\n",
       " \"'s\",\n",
       " 'start',\n",
       " 'our',\n",
       " 'text',\n",
       " 'mine',\n",
       " 'class',\n",
       " '!']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para = \"Hello everyone. It's good to see you. Let's start our text mining class!\"\n",
    "tokens = word_tokenize(para)\n",
    "result = [stemmer.stem(token) for token in tokens]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cook', 'cookery', 'cookbook')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = LancasterStemmer()\n",
    "stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cooking', 'cook', 'cookery', 'cookbook')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('cooking'), lemmatizer.lemmatize('cooking', pos='v'), lemmatizer.lemmatize('cookery'), lemmatizer.lemmatize('cookbooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 'NNP'),\n",
       " ('everyone', 'NN'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('good', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('see', 'VB'),\n",
       " ('you', 'PRP'),\n",
       " ('.', '.'),\n",
       " ('Let', 'VB'),\n",
       " (\"'s\", 'POS'),\n",
       " ('start', 'VB'),\n",
       " ('our', 'PRP$'),\n",
       " ('text', 'NN'),\n",
       " ('mining', 'NN'),\n",
       " ('class', 'NN'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(\"Hello everyone. It's good to see you. Let's start our text mining class!\")\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['everyone', 'good', 'see', 'Let', 'start', 'text', 'mining', 'class']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tag_set = ['NN', 'VB', 'JJ']\n",
    "my_words = [word for word, tag in nltk.pos_tag(tokens) if tag in my_tag_set]\n",
    "my_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '''절망의 반대가 희망은 아니다.\n",
    "어두운 밤하늘에 별이 빛나듯\n",
    "희망은 절망 속에 싹트는 거지\n",
    "만약에 우리가 희망함이 적다면\n",
    "그 누가 세상을 비출어줄까.\n",
    "정희성, 희망 공부'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['절망',\n",
       " '의',\n",
       " '반대',\n",
       " '가',\n",
       " '희망',\n",
       " '은',\n",
       " '아니다',\n",
       " '.',\n",
       " '\\n',\n",
       " '어',\n",
       " '두운',\n",
       " '밤하늘',\n",
       " '에',\n",
       " '별',\n",
       " '이',\n",
       " '빛나듯',\n",
       " '\\n',\n",
       " '희망',\n",
       " '은',\n",
       " '절망',\n",
       " '속',\n",
       " '에',\n",
       " '싹트는',\n",
       " '거지',\n",
       " '\\n',\n",
       " '만약',\n",
       " '에',\n",
       " '우리',\n",
       " '가',\n",
       " '희망',\n",
       " '함',\n",
       " '이',\n",
       " '적다면',\n",
       " '\\n',\n",
       " '그',\n",
       " '누가',\n",
       " '세상',\n",
       " '을',\n",
       " '비출어줄까',\n",
       " '.',\n",
       " '\\n',\n",
       " '정희성',\n",
       " ',',\n",
       " '희망',\n",
       " '공부']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.morphs(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('절망', 'Noun'),\n",
       " ('의', 'Josa'),\n",
       " ('반대', 'Noun'),\n",
       " ('가', 'Josa'),\n",
       " ('희망', 'Noun'),\n",
       " ('은', 'Josa'),\n",
       " ('아니다', 'Adjective'),\n",
       " ('.', 'Punctuation'),\n",
       " ('\\n', 'Foreign'),\n",
       " ('어', 'Noun'),\n",
       " ('두운', 'Noun'),\n",
       " ('밤하늘', 'Noun'),\n",
       " ('에', 'Josa'),\n",
       " ('별', 'Noun'),\n",
       " ('이', 'Josa'),\n",
       " ('빛나듯', 'Verb'),\n",
       " ('\\n', 'Foreign'),\n",
       " ('희망', 'Noun'),\n",
       " ('은', 'Josa'),\n",
       " ('절망', 'Noun'),\n",
       " ('속', 'Noun'),\n",
       " ('에', 'Josa'),\n",
       " ('싹트는', 'Verb'),\n",
       " ('거지', 'Noun'),\n",
       " ('\\n', 'Foreign'),\n",
       " ('만약', 'Noun'),\n",
       " ('에', 'Josa'),\n",
       " ('우리', 'Noun'),\n",
       " ('가', 'Josa'),\n",
       " ('희망', 'Noun'),\n",
       " ('함', 'Noun'),\n",
       " ('이', 'Josa'),\n",
       " ('적다면', 'Verb'),\n",
       " ('\\n', 'Foreign'),\n",
       " ('그', 'Noun'),\n",
       " ('누가', 'Noun'),\n",
       " ('세상', 'Noun'),\n",
       " ('을', 'Josa'),\n",
       " ('비출어줄까', 'Verb'),\n",
       " ('.', 'Punctuation'),\n",
       " ('\\n', 'Foreign'),\n",
       " ('정희성', 'Noun'),\n",
       " (',', 'Punctuation'),\n",
       " ('희망', 'Noun'),\n",
       " ('공부', 'Noun')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.pos(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
