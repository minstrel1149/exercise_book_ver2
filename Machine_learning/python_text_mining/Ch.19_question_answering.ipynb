{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizerFast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce805bcd065486eb67b20d71136a31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)distilled-squad/resolve/main/config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:138: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\admin\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0fbac33a4df4a8bbf82e0a89807e9b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae9859571e342988b6082010682fd77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)squad/resolve/main/tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64aaefac989400aa7e22afbe6066704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)d-distilled-squad/resolve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c87bb1f004947a0ad18e43b0899c79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)tilled-squad/resolve/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the perspectives of text mining? information extraction, data mining, and a KDD information extraction, data mining, and a KDD\n"
     ]
    }
   ],
   "source": [
    "context = r'''Text mining, also referred to as text data mining (abbr.: TDM), similar to text analytics, \n",
    "        is the process of deriving high-quality information from text. It involves \n",
    "        \"the discovery by computer of new, previously unknown information, \n",
    "        by automatically extracting information from different written resources.\" \n",
    "        Written resources may include websites, books, emails, reviews, and articles. \n",
    "        High-quality information is typically obtained by devising patterns and trends \n",
    "        by means such as statistical pattern learning. According to Hotho et al. (2005)\n",
    "        we can distinguish between three different perspectives of text mining: \n",
    "        information extraction, data mining, and a KDD (Knowledge Discovery in Databases) process.''' \n",
    "question = \"What is text mining?\"\n",
    "answer = question_answerer(question=question, context=context)\n",
    "question2 = \"What are the perspectives of text mining?\"\n",
    "answer2 = question_answerer(question=question2, context=context)\n",
    "print(question2, answer2['answer'], context[answer2['start']:answer2['end']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.045622773468494415,\n",
       " 'start': 688,\n",
       " 'end': 734,\n",
       " 'answer': 'information extraction, data mining, and a KDD'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.models.distilbert.tokenization_distilbert_fast.DistilBertTokenizerFast,\n",
       " transformers.models.distilbert.modeling_distilbert.DistilBertForQuestionAnswering)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model.to(device)\n",
    "type(tokenizer), type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.QuestionAnsweringModelOutput"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(question, context, return_tensors='pt').to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "type(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.4038, -3.3430, -5.6367, -4.8551, -6.5684, -2.9705, -4.3552,  3.1155,\n",
       "          -2.5916, -5.0024, -4.8170, -3.7806, -6.6775, -4.9860,  2.4519, -1.9674,\n",
       "          -3.6513, -1.6536, -0.2854, -4.4726, -5.4827, -5.9747, -3.9676, -0.4316,\n",
       "          -5.2797, -4.3061, -2.4209,  0.4658, -4.8278,  1.4840, -2.9245, -3.8173,\n",
       "          -4.7421, -3.0905,  8.1085,  9.8480,  8.7540,  5.3854,  8.2028,  2.0492,\n",
       "           6.1411,  0.1015,  1.7351,  1.5868, -0.8263,  0.7296, -1.7401,  5.6464,\n",
       "           4.5631,  5.1374,  5.6813,  3.8607, -2.1952,  0.4756, -3.0272, -0.0286,\n",
       "          -3.1924, -1.2819, -1.9946, -1.5327, -4.0233,  0.1437,  2.5725,  0.6836,\n",
       "          -3.5727, -2.0314, -4.0234, -1.2922, -1.6372, -2.5505, -4.1987, -3.4589,\n",
       "           0.9349, -2.7395, -4.2431, -5.0164, -2.5822, -6.8693, -4.1553, -6.7693,\n",
       "          -4.3143, -6.6445, -4.3907, -6.8115, -6.7873, -4.5170, -4.9223,  0.3575,\n",
       "          -5.3452, -3.8427, -3.5099, -6.2440, -3.8788, -5.2029, -4.2728, -2.0896,\n",
       "          -5.6316, -5.5557, -4.1067, -7.1451, -4.8306, -5.6256, -5.0897, -6.9333,\n",
       "          -7.3034, -1.7543, -3.7213, -4.6215, -6.3262, -3.6965, -6.7316, -3.5311,\n",
       "          -6.7042, -6.2418, -6.3916, -7.0351, -5.6239, -4.0632, -6.4906, -4.1017,\n",
       "          -5.4902, -4.9735, -4.8868, -3.0635, -4.2650, -3.9012, -6.7928, -2.7636,\n",
       "          -5.4071, -5.5924, -0.2818, -4.8553, -5.8475, -1.5624, -4.8514, -6.6096,\n",
       "          -6.6367, -3.1945, -2.7754, -6.5440, -6.6747, -5.1607, -2.1735, -5.6848,\n",
       "          -5.8650, -4.5885, -6.4783, -6.1690, -5.3887, -6.5383, -4.3551]]),\n",
       " tensor([[-0.2180, -3.8529, -6.8432, -7.0939, -4.6453, -3.3912, -4.5688, -2.5457,\n",
       "          -0.0341, -2.3612, -5.3594, -5.4190, -4.9116, -7.5969, -3.7997, -1.6300,\n",
       "           1.2178, -5.5747, -5.6780, -3.9522, -3.5554, -3.9095, -4.9217, -3.4256,\n",
       "           1.1122,  1.8449, -0.6997, -3.9268, -5.6643, -3.6670, -4.8518, -3.6514,\n",
       "           2.3708,  1.8910, -0.1272, -0.0863,  1.4002, -1.5350, -0.7272,  2.7181,\n",
       "           0.2283, -0.3413,  3.8109,  8.3240,  2.5347, 10.5032,  7.7162, -1.8350,\n",
       "          -1.3735, -2.3497, -2.9121, -0.2978, -2.9504,  2.0628, -2.0113,  0.4314,\n",
       "          -0.1682, -1.4240,  1.6485,  7.4454,  3.6208, -2.5884, -1.4654, -1.0061,\n",
       "          -0.5273,  3.5776, -0.8202, -0.2072, -0.2841,  7.7974,  4.9748,  6.4633,\n",
       "          -3.1768, -0.6390, -5.1702, -5.8609, -2.9603, -5.0950, -3.7655, -5.0186,\n",
       "          -3.4694, -4.7836, -3.0177, -3.7697, -5.6589, -0.3302, -1.2245, -4.2280,\n",
       "          -5.1448, -2.5085, -1.6231, -5.9543, -5.9749, -3.7968, -6.4180, -5.7835,\n",
       "          -4.5897, -4.5492, -2.5101, -6.2753, -1.1947, -6.1993, -4.2558, -5.7969,\n",
       "          -6.6982, -4.6398, -4.0223,  0.2081, -2.0442, -5.4150, -6.3260, -5.7250,\n",
       "          -4.4910, -5.7495, -3.6843, -3.5661, -6.0729, -3.8452, -3.9140, -5.5561,\n",
       "          -6.4942, -5.6781, -6.2276, -4.1746, -4.8569, -2.4178, -6.6831, -5.1681,\n",
       "          -1.4945, -5.4368, -3.9845, -1.2501, -5.0179, -4.2250, -0.8651, -3.7187,\n",
       "          -6.7528, -5.6607, -5.3935, -5.5577, -1.5456, -5.9853, -5.2323, -2.7359,\n",
       "          -5.3649, -4.3879, -1.6323, -2.0100, -0.7068, -1.7889, -4.5686]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.start_logits, outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1327,  1110,  3087,  5463,   136,   102, 18430,  5463,   117,\n",
       "          1145,  2752,  1106,  1112,  3087,  2233,  5463,   113,   170, 13834,\n",
       "          1197,   119,   131, 15439,  2107,   114,   117,  1861,  1106,  3087,\n",
       "         24443, 23894,  1116,   117,  1110,  1103,  1965,  1104,  4167, 21877,\n",
       "          1344,   118,  3068,  1869,  1121,  3087,   119,  1135,  6808,   107,\n",
       "          1103,  6004,  1118,  2775,  1104,  1207,   117,  2331,  3655,  1869,\n",
       "           117,  1118,  7743, 16143,  1158,  1869,  1121,  1472,  1637,  3979,\n",
       "           119,   107, 13404,  3979,  1336,  1511, 12045,   117,  2146,   117,\n",
       "         24853,   117,  3761,   117,  1105,  4237,   119,  1693,   118,  3068,\n",
       "          1869,  1110,  3417,  3836,  1118,  1260,  9356,  1158,  6692,  1105,\n",
       "         14652,  1118,  2086,  1216,  1112, 11435,  4844,  3776,   119,  1792,\n",
       "          1106,  4126,  5114,  3084,  2393,   119,   113,  1478,   114,  1195,\n",
       "          1169, 10706,  1206,  1210,  1472, 22168,  1104,  3087,  5463,   131,\n",
       "          1869, 16026,   117,  2233,  5463,   117,  1105,   170,   148,  2137,\n",
       "          2137,   113, 14966, 11250,  1107, 22929,  1116,   114,  1965,   119,\n",
       "           102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_start_scores = outputs.start_logits\n",
    "answer_end_scores = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(35), tensor(46))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_start = torch.argmax(answer_start_scores)\n",
    "answer_end = torch.argmax(answer_end_scores) + 1\n",
    "answer_start, answer_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the process of deriving high - quality information from text'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'der',\n",
       " '##iving',\n",
       " 'high',\n",
       " '-',\n",
       " 'quality',\n",
       " 'information',\n",
       " 'from',\n",
       " 'text']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
