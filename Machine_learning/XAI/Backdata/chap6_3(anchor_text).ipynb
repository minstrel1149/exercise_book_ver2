{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb6cb5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # surpressing some transformers' output\n",
    "import spacy\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from alibi.explainers import AnchorText\n",
    "from alibi.datasets import fetch_movie_sentiment\n",
    "from alibi.utils import DistilbertBaseUncased, BertBaseUncased, RobertaBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ec63a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = fetch_movie_sentiment()\n",
    "movies.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b2a940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'positive']\n"
     ]
    }
   ],
   "source": [
    "data = movies.data\n",
    "labels = movies.target\n",
    "target_names = movies.target_names\n",
    "print(target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9c30149",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, train_labels, test_labels = train_test_split(data, labels,\n",
    "                                                   test_size=.2,random_state=42)\n",
    "train, val, train_labels, val_labels = train_test_split(train, train_labels, test_size=.1,\n",
    "                                                               random_state=42)\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "val_labels = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2cab2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train)\n",
    "\n",
    "np.random.seed(0)\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "clf.fit(vectorizer.transform(train), train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eff1e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fn = lambda x: clf.predict(vectorizer.transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb2561fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.980\n",
      "Validation accuracy: 0.754\n",
      "Test accuracy: 0.759\n"
     ]
    }
   ],
   "source": [
    "preds_train = predict_fn(train)\n",
    "preds_val = predict_fn(val)\n",
    "preds_test = predict_fn(test)\n",
    "print('Train accuracy: %.3f' % accuracy_score(train_labels, preds_train))\n",
    "print('Validation accuracy: %.3f' % accuracy_score(val_labels, preds_val))\n",
    "print('Test accuracy: %.3f' % accuracy_score(test_labels, preds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6faa2468",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8db4a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Text: a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification .\n",
      "* Prediction: negative\n"
     ]
    }
   ],
   "source": [
    "text = data[4]\n",
    "print(\"* Text: %s\" % text)\n",
    "pred = target_names[predict_fn([text])[0]]\n",
    "alternative = target_names[1 - predict_fn([text])[0]]\n",
    "print(\"* Prediction: %s\" % pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71d69c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = AnchorText(predictor=predict_fn,sampling_strategy='unknown',nlp=nlp)\n",
    "explanation = explainer.explain(text, threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa4a5567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: flashy\n",
      "Precision: 0.99\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "a UNK flashy UNK UNK opaque and emotionally vapid exercise in style UNK mystification .\n",
      "a UNK flashy UNK UNK UNK and emotionally UNK exercise UNK UNK and UNK UNK\n",
      "a UNK flashy UNK narratively opaque UNK UNK UNK exercise in style and UNK UNK\n",
      "UNK visually flashy UNK narratively UNK and emotionally UNK UNK UNK UNK UNK mystification .\n",
      "UNK UNK flashy UNK UNK opaque and emotionally UNK UNK in UNK and UNK .\n",
      "a visually flashy but UNK UNK and UNK UNK UNK in style UNK mystification .\n",
      "a visually flashy but UNK opaque UNK emotionally vapid UNK in UNK and mystification .\n",
      "a UNK flashy but narratively UNK UNK emotionally vapid exercise in style UNK mystification UNK\n",
      "a UNK flashy but narratively opaque UNK emotionally vapid exercise in style and mystification .\n",
      "a visually flashy UNK UNK opaque UNK UNK UNK exercise in UNK UNK UNK .\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "UNK UNK flashy but narratively UNK and UNK UNK UNK in style and UNK UNK\n"
     ]
    }
   ],
   "source": [
    "print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n",
    "print('Precision: %.2f' % explanation.precision)\n",
    "print('\\nExamples where anchor applies and model predicts %s:' % pred)\n",
    "print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_true']]))\n",
    "print('\\nExamples where anchor applies and model predicts %s:' % alternative)\n",
    "print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_false']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a92ce865",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = AnchorText(predictor=predict_fn, sampling_strategy='similarity',\n",
    "                       nlp=nlp, sample_proba=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55530eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: exercise AND vapid\n",
      "Precision: 0.99\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "that visually flashy but tragically opaque and emotionally vapid exercise under genre and glorification .\n",
      "another provably flashy but hysterically bulky and emotionally vapid exercise arounds style and authorization .\n",
      "that- visually flashy but narratively opaque and politically vapid exercise in style and mystification .\n",
      "a unintentionally decal but narratively thick and emotionally vapid exercise in unflattering and mystification .\n",
      "the purposely flashy but narratively rosy and emotionally vapid exercise in style and mystification .\n",
      "thievery intentionally flashy but hysterically gray and anally vapid exercise in style and mystification .\n",
      "a irrationally flashy but narratively smoothness and purposefully vapid exercise near style and diction .\n",
      "a medio flashy but narratively blue and economically vapid exercise since style and intuition .\n",
      "a visually flashy but narratively opaque and anally vapid exercise onwards style and mystification .\n",
      "each purposefully flashy but narratively gorgeous and emotionally vapid exercise in style and mystification .\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "a visually punchy but tragically opaque and hysterically vapid exercise in minimalist and mystification .\n",
      "a visually discernible but realistically posh and physically vapid exercise around style and determination .\n"
     ]
    }
   ],
   "source": [
    "explanation = explainer.explain(text, threshold=0.95)\n",
    "\n",
    "print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n",
    "print('Precision: %.2f' % explanation.precision)\n",
    "print('\\nExamples where anchor applies and model predicts %s:' % pred)\n",
    "print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_true']]))\n",
    "print('\\nExamples where anchor applies and model predicts %s:' % alternative)\n",
    "print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_false']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff100c96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
