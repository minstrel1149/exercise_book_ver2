import os
import streamlit as st

from langchain_community.document_loaders import PyPDFLoader
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableWithMessageHistory

# PDF ì²˜ë¦¬ í•¨ìˆ˜
@st.cache_resource
def process_pdf():
    loader = PyPDFLoader('../data/2024_KB_ë¶€ë™ì‚°_ë³´ê³ ì„œ_ìµœì¢….pdf')
    documents = loader.load()
    text_splitter = SemanticChunker(embeddings=OpenAIEmbeddings())
    chunks = text_splitter.split_documents(documents)
    
    return chunks

# ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
@st.cache_resource
def initialize_vectordb():
    chunks = process_pdf()
    vectordb = Chroma.from_documents(documents=chunks, embedding=OpenAIEmbeddings(), persist_directory='./chroma')

    return vectordb

# ì²´ì¸ ì´ˆê¸°í™”
@st.cache_resource
def initialize_chain():
    vectordb = initialize_vectordb()
    retriever = vectordb.as_retriever(search_kwargs={'k':3})

    template = '''ë‹¹ì‹ ì€ KB ë¶€ë™ì‚° ë³´ê³ ì„œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
    
    ì»¨í…ìŠ¤íŠ¸: {context}'''

    prompt = ChatPromptTemplate.from_messages(
        [('system', template),
         ('placeholder', '{chat_history}'),
         ('human', '{question}')]
    )

    model = ChatOpenAI(model='gpt-5-nano', temperature=0)

    def format_docs(docs):
        return '\n\n'.join(doc.page_content for doc in docs)
    
    base_chain = (
        RunnablePassthrough.assign(context=lambda x: format_docs(retriever.invoke(x['question'])))
        | prompt
        | model
        | StrOutputParser()
    )

    chain_with_history = RunnableWithMessageHistory(
        base_chain,
        lambda session_id: ChatMessageHistory(),
        input_messages_key='question',
        history_messages_key='chat_history'
    )

    return chain_with_history

# Streamlit UI
def main():
    st.set_page_config(page_title='KB ë¶€ë™ì‚° ë³´ê³ ì„œ ì±—ë´‡', page_icon='ğŸ ')
    st.title('ğŸ  KB ë¶€ë™ì‚° ë³´ê³ ì„œ AI ì–´ë“œë°”ì´ì €')
    st.caption('2024 KB ë¶€ë™ì‚° ë³´ê³ ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ')

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    
    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message['role']):
            st.markdown(message['content'])
    
    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if prompt := st.chat_input('ë¶€ë™ì‚° ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.'):
        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
        with st.chat_message('user'):
            st.markdown(prompt)
        st.session_state.messages.append({'role':'user', 'content':prompt})

        # ì²´ì¸ ì´ˆê¸°í™”
        chain = initialize_chain()

        # AI ì‘ë‹µ ìƒì„±
        with st.chat_message('assistant'):
            with st.spinner('ë‹µë³€ ìƒì„± ì¤‘..'):
                response = chain.invoke(
                    {'question':prompt},
                    {'configurable':{'session_id':'streamlit_session'}}
                )
                st.markdown(response)
        
        st.session_state.messages.append({'role':'assistant', 'content':response})

if __name__ == '__main__':
    main()
