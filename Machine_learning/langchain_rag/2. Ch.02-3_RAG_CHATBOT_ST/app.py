import os
import streamlit as st

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableWithMessageHistory
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai import ChatOpenAI
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma

model = ChatOpenAI(model='gpt-5-mini', temperature=0)
embeddings = HuggingFaceEmbeddings(model='BAAI/bge-m3', model_kwargs={'device':'cuda'}, encode_kwargs={'batch_size':8})

def format_docs(docs):
    return '\n\n'.join(doc.page_content for doc in docs)

@st.cache_resource
def process_pdf():
    loader = PyPDFLoader('../data/2024_KB_ë¶€ë™ì‚°_ë³´ê³ ì„œ_ìµœì¢….pdf')
    docs = loader.load()
    full_text = format_docs(docs)
    text_splitter = SemanticChunker(embeddings=embeddings)
    docs = text_splitter.create_documents([full_text])
    for doc in docs:
        doc.metadata['source'] = '2024_KB_ë¶€ë™ì‚°_ë³´ê³ ì„œ_ìµœì¢….pdf'
    
    return docs

@st.cache_resource
def initialize_vectorstore():
    docs = process_pdf()

    return Chroma.from_documents(documents=docs, embedding=embeddings)

@st.cache_resource
def initialize_chain():
    vectorstore = initialize_vectorstore()
    retriever = vectorstore.as_retriever(search_kwargs={'k':3})

    template = '''
    ë‹¹ì‹ ì€ KB ë¶€ë™ì‚° ë³´ê³ ì„œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
    ì •ë³´ì— ì—†ëŠ” ë‚´ìš©ì€ "ì •ë³´ê°€ ì—†ì–´ ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤." ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.
    ì»¨í…ìŠ¤íŠ¸: {context}
    '''
    prompt = ChatPromptTemplate.from_messages(
        [('system', template),
         ('placeholder', '{chat_history}'),
         ('human', '{question}')]
    )

    base_chain = (
        RunnablePassthrough.assign(context=lambda x: format_docs(retriever.invoke(x['question'])))
        | prompt
        | model
        | StrOutputParser()
    )
    chain_with_memory = RunnableWithMessageHistory(
        base_chain,
        lambda session_id: ChatMessageHistory(),
        input_messages_key='question',
        history_messages_key='chat_history'
    )

    return chain_with_memory

def main():
    st.set_page_config(page_title='KB ë¶€ë™ì‚° ë³´ê³ ì„œ ì±—ë´‡', page_icon='ğŸ ')
    st.title('ğŸ  KB ë¶€ë™ì‚° ë³´ê³ ì„œ AI ì–´ë“œë°”ì´ì €')
    st.caption('2024 KB ë¶€ë™ì‚° ë³´ê³ ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ')

    if 'messages' not in st.session_state:
        st.session_state.messages = []
    
    for message in st.session_state.messages:
        with st.chat_message(message['role']):
            st.markdown(message['content'])
    
    if prompt := st.chat_input('ë¶€ë™ì‚° ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.'):
        with st.chat_message('user'):
            st.markdown(prompt)
        st.session_state.messages.append({'role':'user', 'content':prompt})

        chain = initialize_chain()

        with st.chat_message('assistant'):
            with st.spinner('ë‹µë³€ ìƒì„± ì¤‘..'):
                response = chain.invoke(
                    {'question':prompt},
                    {'configurable':{'session_id':'streamlit_session'}}
                )
                st.markdown(response)
        
        st.session_state.messages.append({'role':'assistant', 'content':response})

if __name__ == '__main__':
    main()