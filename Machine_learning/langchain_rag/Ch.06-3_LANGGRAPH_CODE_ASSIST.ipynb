{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd837557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from typing import List, TypedDict\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f906a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://docs.langchain.com/oss/python/langchain/overview#langchain-expression-language-lcel\"\n",
    "\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url,\n",
    "    max_depth=20,\n",
    "    extractor=lambda x: BS(x, 'html.parser').text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68bf967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()\n",
    "docs_sorted = sorted(docs, key=lambda x: x.metadata['source'])[:70]\n",
    "docs_reversed = list(reversed(docs_sorted))\n",
    "\n",
    "concatenated_content = '\\n\\n\\n --- \\n\\n\\n'.join(doc.page_content for doc in docs_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e958c556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14eb38d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = '''\n",
    "당신은 LCEL(Langchain Expression Language) 전문가인 코딩 어시스턴트입니다.\n",
    "다음은 필요한 LCEL 문서 전문입니다.:\n",
    "-------------------\n",
    "{context}\n",
    "-------------------\n",
    "위에 제공된 문서를 기반으로 사용자 질문에 답변하세요.\n",
    "제공하는 코드는 실행 가능해야 하며, 필요한 모든 import문과 변수들이 정의되어 있어야 합니다.\n",
    "답변을 다음과 같은 구조로 작성하세요.:\n",
    "1. prefix: 문제와 접근 방식에 대한 설명\n",
    "2. imports: 코드 블록 import문\n",
    "3. code: import문을 제외한 코드 블록\n",
    "4. description: 질문에 대한 코드 스키마\n",
    "\n",
    "다음은 사용자의 질문입니다.:\n",
    "'''\n",
    "\n",
    "code_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [('system', system), ('placeholder', \"{messages}\")]\n",
    ")\n",
    "\n",
    "llm_gen = ChatOpenAI(model='gpt-4o-mini', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af14e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Code(BaseModel):\n",
    "    prefix: str = Field(description='문제와 접근 방식에 대한 설명')\n",
    "    imports: str = Field(description='코드 블록 import문')\n",
    "    code: str = Field(description='import문을 제외한 코드 블록')\n",
    "    description: str = Field(description='질문에 대한 코드 스키마')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4e7145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_gen_chain = code_gen_prompt | llm_gen.with_structured_output(Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef035550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG(정보 검색 기반 생성) 체인은 정보 검색과 생성 모델을 결합하여 사용자가 질문할 때 관련 정보를 검색하고 그 정보를 바탕으로 응답을 생성하는 방식입니다. LCEL을 사용하여 RAG 체인을 구축하는 방법은 다음과 같습니다.\n",
      "from langchain.chains import RetrievalQA\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.vectorstores import FAISS\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.document_loaders import TextLoader\n",
      "from langchain.text_splitter import CharacterTextSplitter\n",
      "\n",
      "# 필요한 라이브러리들을 import 합니다.\n",
      "# 1. 문서 로드 및 전처리\n",
      "loader = TextLoader('path/to/your/documents')  # 문서 경로를 지정합니다.\n",
      "documents = loader.load()\n",
      "\n",
      "# 2. 텍스트 분할\n",
      "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
      "texts = text_splitter.split_documents(documents)\n",
      "\n",
      "# 3. 임베딩 생성\n",
      "embeddings = OpenAIEmbeddings()\n",
      "\n",
      "# 4. 벡터 스토어 생성\n",
      "vectorstore = FAISS.from_documents(texts, embeddings)\n",
      "\n",
      "# 5. RAG 체인 생성\n",
      "llm = OpenAI(model_name='gpt-3.5-turbo')\n",
      "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=vectorstore.as_retriever())\n",
      "\n",
      "# 6. 질문에 대한 응답 생성\n",
      "query = \"What is the capital of France?\"\n",
      "response = qa_chain.run(query)\n",
      "print(response)\n",
      "이 코드는 LCEL을 사용하여 RAG 체인을 구축하는 방법을 보여줍니다. 문서를 로드하고, 텍스트를 분할한 후, OpenAI 임베딩을 사용하여 벡터 스토어를 생성합니다. 마지막으로, 사용자가 입력한 질문에 대해 관련 정보를 검색하고 응답을 생성합니다.\n"
     ]
    }
   ],
   "source": [
    "question = 'LCEL로 RAG 체인을 어떻게 만들어?'\n",
    "solution = code_gen_chain.invoke(\n",
    "    {'context':concatenated_content, 'messages':[('user', question)]}\n",
    ")\n",
    "print(solution.prefix)\n",
    "print(solution.imports)\n",
    "print(solution.code)\n",
    "print(solution.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4dd807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    error: str\n",
    "    messages: List\n",
    "    generation: str\n",
    "    iterations: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35eac147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    print('---generate---')\n",
    "\n",
    "    messages = state['messages']\n",
    "    iterations = state['iterations']\n",
    "    error = state.get('error', 'no')\n",
    "\n",
    "    if error == 'yes':\n",
    "        messages += [\n",
    "            ('user', '다시 시도해보세요. 출력 결과를 prefix, imports, code block으로 구조화하기 위해 코드 도구를 호출하세요.:')\n",
    "        ]\n",
    "    code_solution = code_gen_chain.invoke(\n",
    "        {'context':concatenated_content, 'messages':messages}\n",
    "    )\n",
    "    messages += [\n",
    "        ('assistant', f'{code_solution.prefix} \\n Imports: {code_solution.imports} \\n Code:{code_solution.code}')\n",
    "    ]\n",
    "    iterations += 1\n",
    "\n",
    "    return {'messages':messages, 'generation': code_solution, 'iterations':iterations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "449d3493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_check(state):\n",
    "    print('---code check---')\n",
    "\n",
    "    messages = state['messages']\n",
    "    code_solution = state['generation']\n",
    "    iterations = state['iterations']\n",
    "\n",
    "    imports = code_solution.imports\n",
    "    code = code_solution.code\n",
    "\n",
    "    try:\n",
    "        exec(imports)\n",
    "    except Exception as e:\n",
    "        print('--import check: Failure--')\n",
    "        error_message = [('user', f'당신의 코드는 import 테스트를 실패했습니다.: {e}')]\n",
    "        messages += error_message\n",
    "\n",
    "        return {'messages':messages, 'generation':code_solution, 'iterations':iterations, 'error':'yes'}\n",
    "    \n",
    "    try:\n",
    "        exec(imports + '\\n' + code)\n",
    "    except:\n",
    "        print('--code block check: Failure')\n",
    "        error_message = [('user', f'당신의 코드는 실행 테스트를 실패했습니다.: {e}')]\n",
    "        messages += error_message\n",
    "\n",
    "        return {'messages':messages, 'generation':code_solution, 'iterations':iterations, 'error':'yes'}\n",
    "    \n",
    "    print('--Success--')\n",
    "\n",
    "    return {'messages':messages, 'generation':code_solution, 'iterations':iterations, 'error':'no'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "307aaf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflected(state):\n",
    "    print('---generate code solution---')\n",
    "\n",
    "    messages = state['messages']\n",
    "    iterations = state['iterations']\n",
    "    code_solution = state['generation']\n",
    "\n",
    "    reflections = code_gen_chain.invoke(\n",
    "        {'context':concatenated_content, 'messages':messages}\n",
    "    )\n",
    "    messages += [('assistant', f'여기 오류를 반영한 코드입니다: {reflections}')]\n",
    "\n",
    "    return {'messages':messages, 'generations':code_solution, 'iterations':iterations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8cc6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
